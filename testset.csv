,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,What are some examples of data brokers exposing social media profiles without permission?,"[""65. See, e.g., Scott Ikeda. Major Data Broker Exposes 235 Million Social Media Profiles in Data Lead: Info\nAppears to Have Been Scraped Without Permission. CPO Magazine. Aug. 28, 2020. https://\nwww.cpomagazine.com/cyber-security/major-data-broker-exposes-235-million-social-media-profiles-\nin-data-leak/; Lily Hay Newman. 1.2 Billion Records Found Exposed Online in a Single Server . WIRED,\nNov. 22, 2019. https://www.wired.com/story/billion-records-exposed-online/\n66.Lola Fadulu. Facial Recognition Technology in Public Housing Prompts Backlash . New York Times.\nSept. 24, 2019.\nhttps://www.nytimes.com/2019/09/24/us/politics/facial-recognition-technology-housing.html\n67. Jo Constantz. ‘They Were Spying On Us’: Amazon, Walmart, Use Surveillance Technology to Bust\nUnions. Newsweek. Dec. 13, 2021.\nhttps://www.newsweek.com/they-were-spying-us-amazon-walmart-use-surveillance-technology-bust-\nunions-1658603\n68. See, e.g., enforcement actions by the FTC against the photo storage app Everalbaum\n(https://www.ftc.gov/legal-library/browse/cases-proceedings/192-3172-everalbum-inc-matter), and\nagainst Weight Watchers and their subsidiary Kurbo(https://www.ftc.gov/legal-library/browse/cases-proceedings/1923228-weight-watchersww)\n69. See, e.g., HIPAA, Pub. L 104-191 (1996); Fair Debt Collection Practices Act (FDCPA), Pub. L. 95-109\n(1977); Family Educational Rights and Privacy Act (FERPA) (20 U.S.C. § 1232g), Children's Online\nPrivacy Protection Act of 1998, 15 U.S.C. 6501–6505, and Confidential Information Protection andStatistical Efficiency Act (CIPSEA) (116 Stat. 2899)\n70. Marshall Allen. You Snooze, You Lose: Insurers Make The Old Adage Literally True . ProPublica. Nov.\n21, 2018.\nhttps://www.propublica.org/article/you-snooze-you-lose-insurers-make-the-old-adage-literally-true\n71.Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Feb. 16, 2012.\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html72. Jack Gillum and Jeff Kao. Aggression Detectors: The Unproven, Invasive Surveillance Technology\nSchools are Using to Monitor Students. ProPublica. Jun. 25, 2019.\nhttps://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology-\nschools-are-using-to-monitor-students/\n73.Drew Harwell. Cheating-detection companies made millions during the pandemic. Now students are\nfighting back. Washington Post. Nov. 12, 2020.\nhttps://www.washingtonpost.com/technology/2020/11/12/test-monitoring-student-revolt/\n74. See, e.g., Heather Morrison. Virtual Testing Puts Disabled Students at a Disadvantage. Government\nTechnology. May 24, 2022.\nhttps://www.govtech.com/education/k-12/virtual-testing-puts-disabled-students-at-a-disadvantage;\nLydia X. Z. Brown, Ridhi Shetty, Matt Scherer, and Andrew Crawford. Ableism And Disability\nDiscrimination In New Surveillance Technologies: How new surveillance technologies in education,\npolicing, health care, and the workplace disproportionately harm disabled people . Center for Democracy\nand Technology Report. May 24, 2022.https://cdt.org/insights/ableism-and-disability-discrimination-in-new-surveillance-technologies-how-new-surveillance-technologies-in-education-policing-health-care-and-the-workplace-disproportionately-harm-disabled-people/\n69""]","Some examples of data brokers exposing social media profiles without permission include a major data broker exposing 235 million social media profiles in a data leak, as reported by CPO Magazine in August 2020, and 1.2 billion records found exposed online in a single server, as reported by WIRED in November 2019.",simple,"[{'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 68}]",True
1,How are unanticipated impacts of GAI systems identified and quantified through regular engagements with AI Actors?,"['28 MAP 5.2:  Practices and personnel for supporting regular engagement with relevant AI Actors  and integrating feedback about \npositive, negative, and unanticipated impacts are in place and documented.  \nAction ID  Suggested Action  GAI Risks  \nMP-5.2-001 Determine context -based measures to identify if new impacts are present due to \nthe GAI system, including regular engagements with downstream AI Actors  to \nidentify and quantify new contexts of unanticipated impacts of GAI systems.  Human -AI Conﬁguration ; Value \nChain and Component Integration  \nMP-5.2-002 Plan regular engagements with AI Actors  responsible for inputs to GAI systems, \nincluding third- party data and algorithms, to review and evaluate unanticipated \nimpacts.  Human -AI Conﬁguration ; Value \nChain and Component Integration  \nAI Actor Tasks:  AI Deployment, AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End -\nUsers, Human Factors, Operation and Monitoring  \n \nMEASURE 1.1:  Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for \nimplementation starting with the most signiﬁcant AI risks. The risks or trustworthiness characteristics that will not – or cannot – be \nmeasured are pro perly documented.  \nAction ID  Suggested Action  GAI Risks  \nMS-1.1-001 Employ methods  to trace the origin and modiﬁcations of digital content . Information Integrity  \nMS-1.1-0 02 Integrate tools designed to analyze content provenance and detect data \nanomalies, verify the authenticity of digital signatures, and identify patterns \nassociated with misinformation or manipulation.  Information Integrity  \nMS-1.1-003 Disaggregate evaluation metrics by demographic factors to identify any \ndiscrepancies in how content provenance mechanisms work across diverse populations.  Information Integrity ; Harmful \nBias and Homogenization  \nMS-1.1-004 Develop a suite of metrics to evaluate structured public feedback exercises  \ninformed by representative AI Actors . Human -AI Conﬁguration ; Harmful \nBias and Homogenization ; CBRN  \nInformation or Capabilities  \nMS-1.1-0 05 Evaluate novel methods and technologies for the measurement of GAI-related \nrisks in cluding in  content provenance , oﬀensive cy ber, and CBRN , while \nmaintaining the models’ ability to produce valid, reliable, and factually accurate outputs.  Information Integrity ; CBRN \nInformation or Capabilities ; \nObscene, Degrading, and/or Abusive Content']","Unanticipated impacts of GAI systems are identified and quantified through regular engagements with AI Actors by determining context-based measures to recognize new impacts caused by the GAI system. This involves engaging with downstream AI Actors to identify and assess new contexts of unanticipated impacts of GAI systems, as well as planning regular engagements with AI Actors responsible for providing inputs to GAI systems, such as third-party data and algorithms, to review and evaluate unanticipated impacts.",simple,"[{'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 31}]",True
2,What is the purpose of implementing regular adversarial testing for GAI systems?,"['25 MP-2.3-002 Review and document accuracy, representativeness, relevance, suitability of data \nused at diﬀerent stages of AI life cycle.  Harmful Bias and Homogenization ; \nIntellectual Property  \nMP-2.3-003 Deploy and document fact -checking techniques to verify the accuracy and \nveracity of information generated by GAI systems, especially when the \ninformation comes from multiple (or unknown) sources.  Information Integrity  \nMP-2.3-004 Develop and implement testing techniques to identify GAI produced content (e.g., synthetic media) that might be indistinguishable from human -generated content.  Information Integrity  \nMP-2.3-005 Implement plans for GAI systems to undergo regular adversarial testing to identify \nvulnerabilities and potential manipulation or misuse.  Information Security  \nAI Actor Tasks:  AI Development, Domain Experts, TEVV  \n \nMAP 3.4:  Processes for operator and practitioner proﬁciency with AI system performance and trustworthiness – and relevant \ntechnical standards and certiﬁcations – are deﬁned, assessed, and documented.  \nAction ID  Suggested Action  GAI Risks  \nMP-3.4-001 Evaluate whether GAI operators and end -users can accurately understand \ncontent lineage and origin.  Human -AI Conﬁguration ; \nInformation Integrity  \nMP-3.4-002 Adapt existing training programs to include modules on digital content \ntransparency.  Information Integrity  \nMP-3.4-003 Develop certiﬁcation programs that test proﬁciency in managing GAI risks and \ninterpreting content provenance, relevant to speciﬁc industry and context.  Information Integrity  \nMP-3.4-004 Delineate human proﬁciency tests from tests of GAI capabilities.  Human -AI Conﬁguration  \nMP-3.4-005 Implement systems to continually monitor and track the outcomes of human- GAI \nconﬁgurations for future reﬁnement and improvements . Human -AI Conﬁguration ; \nInformation Integrity  \nMP-3.4-006 Involve the end -users, practitioners, and operators in GAI system in prototyping \nand testing activities. Make sure these tests cover various scenarios , such as crisis \nsituations or ethically sensitive contexts.  Human -AI Conﬁguration ; \nInformation Integrity ; Harmful Bias \nand Homogenization ; Dangerous , \nViolent, or Hateful Content  \nAI Actor Tasks: AI Design, AI Development, Domain Experts, End -Users, Human Factors, Operation and Monitoring']","Implementing regular adversarial testing for GAI systems helps to identify vulnerabilities and potential manipulation or misuse, ensuring the information security of the systems.",simple,"[{'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 28}]",True
3,How should equity standards be monitored and assessed in systems to ensure adherence and mitigate disparities?,"[""-\ntion when deployed. This assessment should be performed regularly and whenever a pattern of unusual results is occurring. It can be performed using a variety of approaches, taking into account whether and how demographic information of impacted people is available, for example via testing with a sample of users or via qualitative user experience research. Riskier and higher-impact systems should be monitored and assessed more frequentl y. Outcomes of this assessment should include additional disparity mitigation, if needed, or \nfallback to earlier procedures in the case that equity standards are no longer met and can't be mitigated, and prior mechanisms provide better adherence to equity standards. \n27Algorithmic \nDiscrimination \nProtections""]","Equity standards in systems should be monitored and assessed regularly to ensure adherence and mitigate disparities. This assessment can be conducted using various approaches, considering factors such as demographic information of impacted individuals. Riskier and higher-impact systems should be monitored more frequently. The outcomes of the assessment should include additional disparity mitigation measures if necessary, or a fallback to earlier procedures if equity standards are no longer met and cannot be mitigated effectively.",simple,"[{'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 26}]",True
4,How can appropriate diligence on training data use help assess intellectual property and data privacy risks?,"[""27 MP-4.1-0 10 Conduct appropriate diligence on  training data use to assess intellectual property, \nand privacy, risks, including to examine whether use of proprietary or sensitive \ntraining data is consistent with applicable laws.  Intellectual Property ; Data Privacy  \nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third -party entities  \n \nMAP 5.1:  Likelihood and magnitude of each identiﬁed impact (both potentially beneﬁcial and harmful) based on expected use, past \nuses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or d eployed \nthe AI system, or other data are identiﬁed and documented.  \nAction ID  Suggested Action  GAI Risks  \nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \ndata generation capabilities for potential misuse or vulnerabilities . Information Integrity ; Information \nSecurity  \nMP-5.1-002 Identify potential content provenance harms of GAI, such as misinformation or \ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and rank risks based on their likelihood and potential impact, and determine how well provenance solutions address speciﬁc risks and/or harms.  Information Integrity ; Dangerous , \nViolent, or Hateful Content ; \nObscene, Degrading, and/or Abusive Content  \nMP-5.1-003 Consider d isclos ing use of GAI to end user s in relevant contexts, while considering \nthe objective of disclosure, the context of use, the likelihood and magnitude of the  \nrisk posed, the audience of the disclosure, as well as the frequency of the disclosures.  Human -AI Conﬁguration  \nMP-5.1-004 Prioritize GAI structured public feedback processes based on risk assessment estimates.  Information Integrity ; CBRN \nInformation or Capabilities ; \nDangerous , Violent, or Hateful \nContent ; Harmful Bias and \nHomogenization  \nMP-5.1-005 Conduct adversarial role -playing exercises, GAI red -teaming, or chaos testing to \nidentify anomalous or unforeseen failure modes.  Information Security  \nMP-5.1-0 06 Proﬁle threats and negative impacts  arising from GAI systems interacting with, \nmanipulating, or generating content, and outlining known and potential vulnerabilities and the likelihood of their occurrence.  Information Security  \nAI Actor Tasks: AI Deployment, AI Design, AI Development, AI Impact Assessment, Aﬀected Individuals and Communities, End -\nUsers, Operation and Monitoring"", '26 MAP 4.1:  Approaches for mapping AI technology and legal risks of its components – including the use of third -party data or \nsoftware – are in place, followed, and documented, as are risks of infringement of a third -party’s intellectual property or other \nrights.  \nAction ID  Suggested Action  GAI Risks  \nMP-4.1-001 Conduct periodic monitor ing of  AI-generated content for privacy risks; address any \npossible instances of PII or sensitive data exposure.  Data Privacy  \nMP-4.1-002 Implement processes for respondi ng to potential intellectual property infringement \nclaims  or other rights . Intellectual Property  \nMP-4.1-003 Connect new GAI policies, procedures, and processes to existing model, data, \nsoftware development, and IT governance and to legal, compliance, and risk \nmanagement activities . Information Security ; Data Privacy  \nMP-4.1-004 Document training data curation policies, to the extent possible  and according to \napplicable laws and policies . Intellectual Property ; Data Privacy ; \nObscene, Degrading, and/or \nAbusive Content  \nMP-4.1-005 Establish policies for collection, retention, and minimum quality of data, in consideration of the following risks: Disclosure of inappropriate CBRN information ; \nUse of Illegal or dangerous content; Oﬀensive cyber capabilities; Training data \nimbalances that could give rise to harmful biases ; Leak of personally identiﬁable \ninformation, including facial likenesses of individuals.  CBRN Information or Capabilities ; \nIntellectual Property ; Information \nSecurity ; Harmful Bias and \nHomogenization ; Dangerous , \nViolent, or Hateful Content ; Data \nPrivacy  \nMP-4.1-0 06 Implement policies and practices deﬁning how third -party intellectual property and \ntraining data will be used, stored, and protected.  Intellectual Property ; Value Chain \nand Component Integration  \nMP-4.1-0 07 Re-evaluate models that were ﬁne -tuned or enhanced on top of third -party \nmodels.  Value Chain and Component \nIntegration  \nMP-4.1-0 08 Re-evaluate risks when adapting GAI models to new domains.  Additionally, \nestablish warning systems to determine if a GAI system is being used in a new \ndomain where previous assumptions (relating to cont ext of use or mapp ed risks \nsuch as  security, and safety) may no longer hold.  CBRN Information or Capabilities ; \nIntellectual Property ; Harmful Bias \nand Homogenization ; Dangerous , \nViolent, or Hateful Content ; Data \nPrivacy  \nMP-4.1-0 09 Leverage  approaches to detect the presence of  PII or  sensitive data in generated \noutput text, image, video, or audio . Data Privacy']","Appropriate diligence on training data use can help assess intellectual property and data privacy risks by examining whether the use of proprietary or sensitive training data complies with relevant laws. This includes evaluating the likelihood and magnitude of identified impacts, such as potential benefits and harms based on expected use, past incidents, and feedback from external sources. It also involves applying TEVV practices for content provenance, identifying potential harms like misinformation or deepfakes, disclosing GAI use to end users, prioritizing structured public feedback processes, conducting adversarial exercises, profiling threats and negative impacts, monitoring AI-generated content for privacy risks, responding to infringement claims, connecting policies to governance activities, documenting training data curation policies, establishing data collection and retention policies, defining third-party intellectual property usage, re-evaluating models and risks, and leveraging approaches to detect sensitive data in outputs.",simple,"[{'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 30}, {'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 29}]",True
5,Why is a glossary important for transparency in synthetic media with NIST AI framework?,"['57 National Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: \nHow AI Risks Diﬀer from Traditional Software Risks . \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B  \nNational Institute of Standards and Technology  (2023) AI RMF Playbook . \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook  \nNational Institue of Standards and Technology (2023) Framing Risk \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1- sec-risk \nNational Institu te of Standards and Technology (2023) The Language of Trustworthy AI: An In- Depth \nGlossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary  \nNational Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing \nBias in Artiﬁcial Intelligence https://www.nist.gov/publications/towards -standard -identifying -and-\nmanaging- bias-artiﬁcial -intelligence  \nNorthcutt, C. et al. (2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.  \narXiv . https://arxiv.org/pdf/2103.14749  \nOECD (2023) ""Advancing accountability in AI: Governing and managing risks throughout the lifecycle for \ntrustworthy AI"", OECD Digital Economy Papers , No. 349, OECD Publishing,  Paris . \nhttps://doi.org/10.1787/2448f04b- en \nOECD (2024) ""Deﬁning AI incidents and related terms"" OECD Artiﬁcial Intelligence Papers , No. 16, OECD \nPublishing, Paris . https://doi.org/10.1787/d1a8d965- en \nOpenAI  (2023) GPT-4 System Card . https://cdn.openai.com/papers/gpt -4-system -card.pdf  \nOpenAI  (2024) GPT-4 Technical Report. https://arxiv.org/pdf/2303.08774  \nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity?  ICLR . \nhttps://arxiv.org/pdf/2309.05196  \nPark,  P.  et. al. (2024)  AI deception: A survey of  examples, risks, and potential solutions. Patterns, 5(5).  \narXiv . https://arxiv.org/pdf/2308.14752  \nPartnership on AI  (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect \nDisclosure . https://partnershiponai.org/glossary -for-synthetic -media- transparency -methods -part-1-\nindirect -disclosure/  \nQu, Y . et al. (2023) Unsafe Diﬀusion: On the Generation of Unsafe Images and Hateful Memes From Text -\nTo-Image Models.  arXiv . https://arxiv.org/pdf/2305.13873  \nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model \ncompression. PLOS One . https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668  \nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes in Victimization and Perpetration. Sage. \nhttps://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47- 08862605221122834\n \nSandbrink, J. (2023) Artiﬁcial intelligence and biological misuse: Diﬀerentiating risks of language models and biological design tools. arXiv . https://arxiv.org/pdf/2306.13952']","A glossary is important for transparency in synthetic media with the NIST AI framework because it helps define and standardize terms and concepts related to AI and synthetic media, ensuring clear communication and understanding among stakeholders. This clarity is crucial for effective implementation of the NIST AI framework and for addressing risks and challenges associated with synthetic media.",multi_context,"[{'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 60}]",True
6,"How do AI systems and tech affect equity in sectors, and what interventions are needed to address harms?","[""APPENDIX\nPanel 3: Equal Opportunities and Civil Justice. This event explored current and emerging uses of \ntechnology that impact equity of opportunity in employment, education, and housing. \nWelcome : \n•Rashida Richardson, Senior Policy Advisor for Data and Democracy, White House Office of Science and\nTechnology Policy\n•Dominique Harrison, Director for Technology Policy, The Joint Center for Political and EconomicStudies\nModerator: Jenny Yang, Director, Office of Federal Contract Compliance Programs, Department of Labor \nPanelists: \n•Christo Wilson, Associate Professor of Computer Science, Northeastern University\n•Frida Polli, CEO, Pymetrics\n•Karen Levy, Assistant Professor, Department of Information Science, Cornell University\n•Natasha Duarte, Project Director, Upturn\n•Elana Zeide, Assistant Professor, University of Nebraska College of Law\n•Fabian Rogers, Constituent Advocate, Office of NY State Senator Jabari Brisport and Community\nAdvocate and Floor Captain, Atlantic Plaza Towers Tenants Association\nThe individual panelists described the ways in which AI systems and other technologies are increasingly being \nused to limit access to equal opportunities in education, housing, and employment. Education-related concerning uses included the increased use of remote proctoring systems, student location and facial recognition tracking, teacher evaluation systems, robot teachers, and more. Housing-related concerning uses including automated tenant background screening and facial recognition-based controls to enter or exit housing complexes. Employment-related concerning uses included discrimination in automated hiring screening and workplace surveillance. Various panelists raised the limitations of existing privacy law as a key \nconcern, pointing out that students should be able to reinvent themselves and require privacy of their student records and education-related data in order to do so. The overarching concerns of surveillance in these domains included concerns about the chilling effects of surveillance on student expression, inappropriate control of tenants via surveillance, and the way that surveillance of workers blurs the boundary between work and life and exerts extreme and potentially damaging control over workers' lives. Additionally, some panelists pointed out ways that data from one situation was misapplied in another in a way that limited people's opportunities, for example data from criminal justice settings or previous evictions being used to block further access to housing. Throughout, various panelists emphasized that these technologies are being used to shift the burden of oversight and efficiency from employers to workers, schools to students, and landlords to tenants, in ways that diminish and encroach on equality of opportunity; assessment of these technologies should include whether they are genuinely helpful in solving an identified problem. \nIn discussion of technical and governance interventions that that are needed to protect against the harms of these technologies, panelists individually described the importance of: receiving community input into the design and use of technologies, public reporting on crucial elements of these systems, better notice and consent procedures that ensure privacy based on context and use case, ability to opt-out of using these systems and receive a fallback to a human process, providing explanations of decisions and how these systems work, the need for governance including training in using these systems, ensuring the technological use cases are genuinely related to the goal task and are locally validated to work, and the need for institution and protection of third party audits to ensure systems continue to be accountable and valid. \n57""]","AI systems and other technologies are increasingly being used to limit access to equal opportunities in education, housing, and employment. Concerning uses include remote proctoring systems, student location and facial recognition tracking, teacher evaluation systems, robot teachers, automated tenant background screening, facial recognition-based controls for housing complexes, discrimination in automated hiring screening, and workplace surveillance. Panelists emphasized the need for technical and governance interventions to protect against the harms of these technologies, including community input into design, public reporting, better notice and consent procedures, opt-out options, explanations of decisions, governance training, validation of use cases, and third-party audits for accountability.",multi_context,"[{'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 56}]",True
7,"What criteria should automated systems meet for providing notice and explanations, considering risk, validity, and accountability?","[""NOTICE & \nEXPLANATION \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nAn automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and \nexplanations as to how and why a decision was made or an action was taken by the system. These expectations are explained below. \nProvide clear, timely, understandable, and accessible notice of use and explanations \nGenerally accessible plain language documentation. The entity responsible for using the automated \nsystem should ensure that documentation describing the overall system (including any human components) is \npublic and easy to find. The documentation should describe, in plain language, how the system works and how \nany automated component is used to determine an action or decision. It should also include expectations about \nreporting described throughout this framework, such as the algorithmic impact assessments described as \npart of Algorithmic Discrimination Protections. \nAccount able. Notices should clearly identify the entity r esponsible for designing each component of the \nsystem and the entity using it. \nTimely and up-to-date. Users should receive notice of the use of automated systems in advance of using or \nwhile being impacted by the technolog y. An explanation should be available with the decision itself, or soon \nthereafte r. Notice should be kept up-to-date and people impacted by the system should be notified of use case \nor key functionality changes. \nBrief and clear. Notices and explanations should be assessed, such as by research on users’ experiences, \nincluding user testing, to ensure that the people using or impacted by the automated system are able to easily \nfind notices and explanations, read them quickl y, and understand and act on them. This includes ensuring that \nnotices and explanations are accessible to users with disabilities and are available in the language(s) and read-\ning level appropriate for the audience. Notices and explanations may need to be available in multiple forms, \n(e.g., on pape r, on a physical sign, or online), in order to meet these expectations and to be accessible to the \nAmerican public. \nProvide explanations as to how and why a decision was made or an action was taken by an \nautomated system \nTailored to the purpose. Explanations should be tailored to the specific purpose for which the user is \nexpected to use the explanation, and should clearly state that purpose. An informational explanation might differ from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the context of a dispute or contestation process. For the purposes of this framework, 'explanation' should be construed broadly. An explanation need not be a plain-language statement about causality but could consist of any mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the stated purpose. Tailoring should be assessed (e.g., via user experience research). \nTailored to the target of the explanation. Explanations should be targeted to specific audiences and clearly state that audience. An explanation provided to the subject of a decision might differ from one provided to an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e.g., via user experience research). \n43"", ""NOTICE & \nEXPLANATION \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nTailored to the level of risk. An assessment should be done to determine the level of risk of the auto -\nmated system. In settings where the consequences are high as determined by a risk assessment, or extensive \noversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should be built into the system design so that the system’s full behavior can be explained in advance (i.e., only fully transparent models should be used), rather than as an after-the-decision interpretation. In other settings, the extent of explanation provided should be tailored to the risk level. \nValid. The explanation provided by a system should accurately reflect the factors and the influences that led \nto a particular decision, and should be meaningful for the particular customization based on purpose, target, and level of risk. While approximation and simplification may be necessary for the system to succeed based on the explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns related to revealing decision-making information, such simplifications should be done in a scientifically supportable way. Where appropriate based on the explanatory system, error ranges for the explanation should be calculated and included in the explanation, with the choice of presentation of such information balanced with usability and overall interface complexity concerns. \nDemonstrate protections for notice and explanation \nReporting. Summary reporting should document the determinations made based on the above consider -\nations, including: the responsible entities for accountability purposes; the goal and use cases for the system, identified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of the explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment of how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of risk. Individualized profile information should be made readily available to the greatest extent possible that includes explanations for any system impacts or inferences. Reporting should be provided in a clear plain language and machine-readable manner. \n44""]",The answer to given question is not present in context,multi_context,"[{'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 42}, {'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 43}]",True
8,"How can organizations reduce GAI risks through transparency policies and risk assessment, considering factors like information integrity, harmful content, and human-AI configurations?","['14 GOVERN 1.2:  The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices.  \nAction ID  Suggested Action  GAI Risks  \nGV-1.2-001 Establish transparency policies and processes for documenting the origin and \nhistory of training data and generated data for GAI applications  to advance  digital \ncontent transparency , while balancing the proprietary nature of training  \napproaches . Data Privacy ; Information \nIntegrity ; Intellectual Property  \nGV-1.2-0 02 Establish policies to evaluate risk -relevant capabilities of GAI and robustness of \nsafety measures, both prior to deployment and on an ongoing basis, through \ninternal and external evaluations.  CBRN Information or Capabilities ; \nInformation Security  \nAI Actor Tasks: Governance and Oversight  \n \nGOVERN 1.3:  Processes, procedures, and practices are in place to determine the needed level of risk management activities based \non the organization’s risk tolerance.  \nAction ID  Suggested Action  GAI Risks  \nGV-1.3-001 Consider the following  factors when updating or deﬁning risk tiers for GAI: Abuses \nand impacts to information integrity; Dependencies between GAI and other IT or \ndata systems; Harm to fundamental rights or public safety ; Presentation of \nobscene, objectionable, oﬀensive, discrimina tory, invalid or untruthful output; \nPsychological impacts to humans (e.g., anthropomorphization, algorithmic aversion, emotional entanglement); Possibility for malicious use ; Whether the \nsystem introduces  signiﬁcant new security vulnerabilities ; Anticipated system \nimpact on some groups compared to others ; Unreliable decision making \ncapabilities, validity, adaptability, and variability of GAI system performance over \ntime.  Information Integrity ; Obscene, \nDegrading, and/or Abusive \nContent ; Value Chain and \nComponent Integration; Harmful \nBias and Homogenization ; \nDangerous , Violent , or Hateful  \nContent ; CBRN Information or \nCapabilities  \nGV-1.3-002 Establish minimum thresholds for performance or assurance criteria and review as \npart of deployment approval (“go/”no -go”) policies, procedures, and processes, \nwith reviewed processes and approval thresholds reﬂecting measurement of GAI capabilities and risks.  CBRN Information or Capabilities ; \nConfabulation ; Dangerous , \nViolent, or Hateful Content  \nGV-1.3-003 Establish a test plan and response policy, before developing highly capable models, \nto periodically evaluate whether the model may misuse CBRN information or capabilities and/or oﬀensive cyber capabilities.  CBRN Information or Capabilities ; \nInformation Security', '18 GOVERN 3.2:  Policies and procedures are in place to deﬁne and diﬀerentiate roles and responsibilities for human -AI conﬁgurations \nand oversight of AI systems.  \nAction ID  Suggested Action  GAI Risks  \nGV-3.2-001 Policies are in place to bolster oversight of GAI systems with independent \nevaluations  or assessments of GAI models or systems  where the type and \nrobustness of evaluations are proportional to the identiﬁed risks.  CBRN Information or Capabilities ; \nHarmful Bias and Homogenization  \nGV-3.2-002 Consider adjustment of organizational roles and components across lifecycle \nstages of large or complex GAI systems, including: Test and evaluation, validation, \nand red- teaming of GAI systems; GAI content moderation; GAI system \ndevelopment and engineering; Increased accessibility of GAI tools, interfaces, and \nsystems, Incident response and containment.  Human -AI Conﬁguration ; \nInformation Security ; Harmful Bias \nand Homogenization  \nGV-3.2-003 Deﬁne acceptable use policies for GAI interfaces, modalities, and human -AI \nconﬁgurations  (i.e., for chatbots and decision -making tasks) , including  criteria for \nthe kinds of queries GAI applications should refuse to respond to.  Human -AI Conﬁguration  \nGV-3.2-004 Establish policies for user feedback mechanisms for GAI systems  which include \nthorough instructions and any mechanisms for recourse . Human -AI Conﬁguration   \nGV-3.2-005 Engage in threat modeling to anticipate potential risks from GAI systems.  CBRN Information or Capabilities ; \nInformation Security  \nAI Actors: AI Design  \n \nGOVERN 4.1:  Organizational policies and practices are in place to foster a critical thinking and safety -ﬁrst mindset in the design, \ndevelopment, deployment, and uses of AI systems to minimize potential negative impacts.  \nAction ID  Suggested Action  GAI Risks  \nGV-4.1-001 Establish policies and procedures that address continual improvement processes \nfor GAI risk measurement . Address general risks associated with a lack of \nexplainability and transparency in GAI systems by using ample documentation and \ntechniques such as: application of gradient -based attributions, occlusion/term \nreduction, counterfactual prompts and prompt eng ineering, and analysis of \nembeddings; Assess and update risk measurement approaches at regular cadences.  Confabulation  \nGV-4.1-002 Establish policies, procedures, and processes detailing risk measurement in \ncontext of use with standardized measurement protocols and structured public feedback exercises such as AI red -teaming or independent external evaluations . CBRN Information and Capability ; \nValue Chain and Component Integration']","Organizations can reduce GAI risks through transparency policies that document the origin and history of training data and generated data for GAI applications. By establishing transparency processes, organizations can advance digital content transparency while balancing the proprietary nature of training approaches. Additionally, organizations should establish policies to evaluate the risk-relevant capabilities of GAI and the robustness of safety measures both before deployment and on an ongoing basis through internal and external evaluations. These actions can help mitigate risks related to information integrity, harmful content, and human-AI configurations.",multi_context,"[{'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 17}, {'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 21}]",True
9,How can organizations address GAI incidents to comply with legal standards like HIPAA breach reporting and autonomous vehicle crash reporting?,"['46 MG-4.3-003 Report GAI incidents in compliance with legal and regulatory requirements (e.g., \nHIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle \ncrash reporting requirements.  Information Security ; Data Privacy  \nAI Actor Tasks:  AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End -Users, Human Factors, Operation and \nMonitorin g']","Organizations can address GAI incidents to comply with legal standards such as HIPAA breach reporting and autonomous vehicle crash reporting by ensuring that they report incidents in accordance with the specific requirements outlined by regulatory bodies like OCR (2023) for HIPAA breach reporting and NHTSA (2022) for autonomous vehicle crash reporting. This involves understanding the reporting criteria, timelines, and procedures set forth by these regulatory entities to ensure timely and accurate reporting of incidents.",reasoning,"[{'source': '/Users/joenoss/Downloads/NIST.AI.600-1.pdf', 'page': 49}]",True
