{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/joenoss/.local/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-openai in /Users/joenoss/.local/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain-community in /Users/joenoss/.local/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: unstructured in /opt/anaconda3/lib/python3.12/site-packages (0.15.7)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: pdfminer in /opt/anaconda3/lib/python3.12/site-packages (20191125)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: pymupdf in /opt/anaconda3/lib/python3.12/site-packages (1.24.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /Users/joenoss/.local/lib/python3.12/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/joenoss/.local/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.1.128)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.9.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (1.48.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.5.2)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.0.0)\n",
      "Requirement already satisfied: filetype in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (5.2.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: tabulate in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (2.12.1)\n",
      "Requirement already satisfied: python-iso639 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (2024.4.27)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (3.9.7)\n",
      "Requirement already satisfied: backoff in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.11.0)\n",
      "Requirement already satisfied: unstructured-client in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (0.25.8)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (1.14.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from unstructured) (5.9.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: pycryptodome in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer) (3.20.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.10 in /opt/anaconda3/lib/python3.12/site-packages (from pymupdf) (1.24.10)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.2)\n",
      "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: deepdiff>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (8.0.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pypdf>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (4.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: orderly-set==5.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from deepdiff>=6.0->unstructured-client->unstructured) (5.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install langchain langchain-openai langchain-community python-dotenv unstructured faiss-cpu pdfminer tiktoken pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "## set up openai api key\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "## set up openai chat model\n",
    "openai_embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## set up system template\n",
    "system_template = \"\"\"You are an AI assistant specialized in answering questions about the AI Bill of Rights. \n",
    "Your responses should be based on the provided context. If the information isn't in the context, say you don't know.\n",
    "Always strive for accuracy and clarity in your answers.\"\"\"\n",
    "\n",
    "## set up human template\n",
    "human_template = \"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a concise and accurate answer based on the given context.\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55: In discussion of technical and governance interventions that that are needed to protect against the harms of \n",
      "these technologies, various panelists emphasized that transparency is important but is not enough to achieve accountability. Some panelists discussed their individual views on additional sys\n",
      "55: APPENDIX\n",
      "Panelists discussed the benefits of AI-enabled systems and their potential to build better and more \n",
      "innovative infrastructure. They individually noted that while AI technologies may be new, the process of \n",
      "technological diffusion is not, and that it was critical to have thoughtful and resp\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Create FAISS index\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "\n",
    "# Perform similarity search\n",
    "docs = faiss_index.similarity_search(\"How will the community be engaged?\", k=2)\n",
    "\n",
    "# Print results\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up semantic chunker for text splitting\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create the semantic chunker\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Use the same embedding model as in the semantic chunker\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create Qdrant vector store using the loaded and split pages\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    documents=pages,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a ChatOpenAI instance with the GPT-4 model\n",
    "gpt4_chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create the retrieval QA chain\n",
    "retrieval_augmented_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gpt4_chat_model,  # Use the GPT-4 model here\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=qdrant_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": chat_prompt}\n",
    ")\n",
    "\n",
    "# The final_chain line is not necessary if you're using the retrieval_augmented_qa_chain directly\n",
    "# If you want to keep it for consistency, you can do:\n",
    "final_chain = retrieval_augmented_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_augmented_qa_chain.invoke({\"query\" : \"How do we define the riskiness of AI?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How do we define the riskiness of AI?',\n",
      " 'result': 'The context does not provide a specific definition for the '\n",
      "           'riskiness of AI. It discusses various frameworks and principles '\n",
      "           'aimed at managing risks associated with AI, such as the NIST AI '\n",
      "           'Risk Management Framework, which focuses on trustworthiness '\n",
      "           'considerations like accuracy, explainability, reliability, and '\n",
      "           'privacy. However, a precise definition of \"riskiness\" itself is '\n",
      "           'not included.',\n",
      " 'source_documents': [Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 44, '_id': 'ce7213426ee64f0aaae5cdb3d908be89', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content='accuracy), and enable human users to understand, appropriately trust, and effectively manage the emerging \\ngeneration of artificially intelligent partners.95 The National Science Foundation’s program on Fairness in \\nArtificial Intelligence also includes a specific interest in research foundations for explainable AI.96\\n45'),\n",
      "                      Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 21, '_id': 'e013bda23110439e8ad7a3260be7f759', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\nSome U.S government agencies have developed specific frameworks for ethical use of AI \\nsystems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina -\\ntion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the \\nethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence \\nEthical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \\nsecurity and defense activities.21 Similarl y, the U.S. Intelligence Community (IC) has developed the Principles \\nof Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to \\ndevelop and use AI in furtherance of the IC\\'s mission, as well as an AI Ethics Framework to help implement \\nthese principles.22\\nThe National Science Foundation (NSF) funds extensive research to help foster the \\ndevelopment of automated systems that adhere to and advance their safety, security and \\neffectiveness. Multiple NSF programs support research that directly addresses many of these principles: \\nthe National AI Research Institutes23 support research on all aspects of safe, trustworth y, fai r, and explainable \\nAI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe \\nautonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25 \\nprogram supports research on cybersecurity and privacy enhancing technologies in automated systems; the \\nFormal Methods in the Field26 program supports research on rigorous formal verification and analysis of \\nautomated systems and machine learning, and the Designing Accountable Software Systems27 program supports \\nresearch on rigorous and reproducible methodologies for developing software systems with legal and regulatory \\ncompliance in mind. \\nSome state legislatures have placed strong transparency and validity requirements on \\nthe use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a \\ncause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any \\npretrial risk assessment, before use in the state, first be \"shown to be free of bias against any class of \\nindividuals protected from discrimination by state or federal law\", that any locality using a pretrial risk \\nassessment must first formally validate the claim of its being free of bias, that \"all documents, records, and \\ninformation used to build or validate the risk assessment shall be open to public inspection,\" and that assertions \\nof trade secrets cannot be used \"to quash discovery in a criminal matter by a party to a criminal case.\" \\n22'),\n",
      "                      Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 20, '_id': 'e11cd1c32fa44b10beb3d2cf5a269abb', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\nExecutive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government requires that certain federal agencies adhere to nine principles when \\ndesigning, developing, acquiring, or using AI for purposes other than national security or \\ndefense. These principles—while taking into account the sensitive law enforcement and other contexts in which \\nthe federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and \\nrespectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) \\nsafe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar -\\nent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. \\nAffected agencies across the federal government have released AI use case inventories13 and are implementing \\nplans to bring those AI systems into compliance with the Executive Order or retire them. \\nThe law and policy landscape for motor vehicles shows that strong safety regulations—and \\nmeasures to address harms when they occur—can enhance innovation in the context of com-\\nplex technologies. Cars, like automated digital systems, comprise a complex collection of components. \\nThe National Highway Traffic Safety Administration,14 through its rigorous standards and independent \\nevaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to \\ninnovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate \\nrequirements on drivers, such as slowing down near schools or playgrounds.16\\nFrom large companies to start-ups, industry is providing innovative solutions that allow \\norganizations to mitigate risks to the safety and efficacy of AI systems, both before \\ndeployment and through monitoring over time.17 These innovative solutions include risk \\nassessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing \\nmonitoring, documentation procedures specific to model assessments, and many other strategies that aim to \\nmitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety \\nand effectiveness concerns. \\nThe Office of Management and Budget (OMB) has called for an expansion of opportunities \\nfor meaningful stakeholder engagement in the design of programs and services. OMB also \\npoints to numerous examples of effective and proactive stakeholder engagement, including the Community-\\nBased Participatory Research Program developed by the National Institutes of Health and the participatory \\ntechnology assessments developed by the National Oceanic and Atmospheric Administration.18\\nThe National Institute of Standards and Technology (NIST) is developing a risk \\nmanagement framework to better manage risks posed to individuals, organizations, and \\nsociety by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for \\nvoluntary use to help incorporate trustworthiness considerations into the design, development, use, and \\nevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \\ninput. The NIST framework aims to foster the development of innovative approaches to address \\ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy,'),\n",
      "                      Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 64, '_id': '6dce988e98ab4811b542e19f0b37d760', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content=\"ENDNOTES\\n23. National Science Foundation. National Artificial Intelligence Research Institutes. Accessed Sept. 12,\\n2022. https://beta.nsf.gov/funding/opportunities/national-artificial-intelligence-research-institutes\\n24. National Science Foundation. Cyber-Physical Systems. Accessed Sept. 12, 2022. https://beta.nsf.gov/\\nfunding/opportunities/cyber-physical-systems-cps25. National Science Foundation. Secure and Trustworthy Cyberspace. Accessed Sept. 12, 2022. https://\\nbeta.nsf.gov/funding/opportunities/secure-and-trustworthy-cyberspace-satc26. National Science Foundation. Formal Methods in the Field. Accessed Sept. 12, 2022. https://\\nbeta.nsf.gov/funding/opportunities/formal-methods-field-fmitf27. National Science Foundation. Designing Accountable Software Systems. Accessed Sept. 12, 2022.\\nhttps://beta.nsf.gov/funding/opportunities/designing-accountable-software-systems-dass28. The Leadership Conference Education Fund. The Use Of Pretrial “Risk Assessment” Instruments: A\\nShared Statement Of Civil Rights Concerns. Jul. 30, 2018. http://civilrightsdocs.info/pdf/criminal-justice/\\nPretrial-Risk-Assessment-Short.pdf; https://civilrights.org/edfund/pretrial-risk-assessments/\\n29. Idaho Legislature. House Bill 118. Jul. 1, 2019. https://legislature.idaho.gov/sessioninfo/2019/\\nlegislation/H0118/\\n30. See, e.g., Executive Office of the President. Big Data: A Report on Algorithmic Systems, Opportunity, and\\nCivil Rights. May, 2016. https://obamawhitehouse.archives.gov/sites/default/files/microsites/\\nostp/2016_0504_data_discrimination.pdf; Cathy O’Neil. Weapons of Math Destruction. Penguin Books.2017. https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction; Ruha Benjamin. Race After\\nTechnology: Abolitionist Tools for the New Jim Code. Polity. 2019. https://www.ruhabenjamin.com/race-\\nafter-technology\\n31.See, e.g., Kashmir Hill. Another Arrest, and Jail Time, Due to a Bad Facial Recognition Match: A New\\nJersey man was accused of shoplifting and trying to hit an officer with a car. He is the third known Black man\\nto be wrongfully arrested based on face recognition. New York Times. Dec. 29, 2020, updated Jan. 6, 2021.\\nhttps://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html; Khari\\nJohnson. How Wrongful Arrests Based on AI Derailed 3 Men's Lives . Wired. Mar. 7, 2022. https://\\nwww.wired.com/story/wrongful-arrests-ai-derailed-3-mens-lives/\\n32. Student Borrower Protection Center. Educational Redlining. Student Borrower Protection Center\\nReport. Feb. 2020. https://protectborrowers.org/wp-content/uploads/2020/02/Education-Redlining-\\nReport.pdf\\n33. Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women . Reuters. Oct.\\n10, 2018. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-\\nsecret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G34. Todd Feathers. Major Universities Are Using Race as a “High Impact Predictor” of Student Success:\\nStudents, professors, and education experts worry that that’s pushing Black students in particular out of math\\nand science. The Markup. Mar. 2, 2021. https://themarkup.org/machine-learning/2021/03/02/major-\\nuniversities-are-using-race-as-a-high-impact-predictor-of-student-success\\n65\")]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pp(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are some of the principles for ensuring safety in relation to '\n",
      "          'AI?',\n",
      " 'result': 'Some principles for ensuring safety in relation to AI, as outlined '\n",
      "           'in the context, include:\\n'\n",
      "           '\\n'\n",
      "           '1. **Lawful and Respectful**: AI systems must adhere to legal '\n",
      "           'standards and respect national values.\\n'\n",
      "           '2. **Purposeful and Performance-Driven**: AI should have clear '\n",
      "           'objectives and be driven by performance metrics.\\n'\n",
      "           '3. **Accurate, Reliable, and Effective**: AI systems must produce '\n",
      "           'accurate and reliable outcomes.\\n'\n",
      "           '4. **Safe, Secure, and Resilient**: AI should be designed to be '\n",
      "           'safe from vulnerabilities and resilient to failures.\\n'\n",
      "           '5. **Understandable**: The functioning of AI systems should be '\n",
      "           'comprehensible to users.\\n'\n",
      "           '6. **Responsible and Traceable**: There should be accountability '\n",
      "           'in the development and deployment of AI systems.\\n'\n",
      "           '7. **Regularly Monitored**: Continuous monitoring of AI systems is '\n",
      "           'necessary to ensure ongoing safety and effectiveness.\\n'\n",
      "           '8. **Transparent**: AI operations should be transparent to '\n",
      "           'stakeholders.\\n'\n",
      "           '9. **Accountable**: There must be mechanisms in place to hold '\n",
      "           'parties accountable for the use of AI systems. \\n'\n",
      "           '\\n'\n",
      "           'These principles are part of the guidelines established by various '\n",
      "           'federal agencies and are consistent with the Blueprint for an AI '\n",
      "           'Bill of Rights.',\n",
      " 'source_documents': [Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 21, '_id': 'e013bda23110439e8ad7a3260be7f759', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\nSome U.S government agencies have developed specific frameworks for ethical use of AI \\nsystems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina -\\ntion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the \\nethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence \\nEthical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \\nsecurity and defense activities.21 Similarl y, the U.S. Intelligence Community (IC) has developed the Principles \\nof Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to \\ndevelop and use AI in furtherance of the IC\\'s mission, as well as an AI Ethics Framework to help implement \\nthese principles.22\\nThe National Science Foundation (NSF) funds extensive research to help foster the \\ndevelopment of automated systems that adhere to and advance their safety, security and \\neffectiveness. Multiple NSF programs support research that directly addresses many of these principles: \\nthe National AI Research Institutes23 support research on all aspects of safe, trustworth y, fai r, and explainable \\nAI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe \\nautonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25 \\nprogram supports research on cybersecurity and privacy enhancing technologies in automated systems; the \\nFormal Methods in the Field26 program supports research on rigorous formal verification and analysis of \\nautomated systems and machine learning, and the Designing Accountable Software Systems27 program supports \\nresearch on rigorous and reproducible methodologies for developing software systems with legal and regulatory \\ncompliance in mind. \\nSome state legislatures have placed strong transparency and validity requirements on \\nthe use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a \\ncause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any \\npretrial risk assessment, before use in the state, first be \"shown to be free of bias against any class of \\nindividuals protected from discrimination by state or federal law\", that any locality using a pretrial risk \\nassessment must first formally validate the claim of its being free of bias, that \"all documents, records, and \\ninformation used to build or validate the risk assessment shall be open to public inspection,\" and that assertions \\nof trade secrets cannot be used \"to quash discovery in a criminal matter by a party to a criminal case.\" \\n22'),\n",
      "                      Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 20, '_id': 'e11cd1c32fa44b10beb3d2cf5a269abb', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\\nReal-life examples of how these principles can become reality, through laws, policies, and practical \\ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \\nExecutive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \\nFederal Government requires that certain federal agencies adhere to nine principles when \\ndesigning, developing, acquiring, or using AI for purposes other than national security or \\ndefense. These principles—while taking into account the sensitive law enforcement and other contexts in which \\nthe federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and \\nrespectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) \\nsafe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar -\\nent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. \\nAffected agencies across the federal government have released AI use case inventories13 and are implementing \\nplans to bring those AI systems into compliance with the Executive Order or retire them. \\nThe law and policy landscape for motor vehicles shows that strong safety regulations—and \\nmeasures to address harms when they occur—can enhance innovation in the context of com-\\nplex technologies. Cars, like automated digital systems, comprise a complex collection of components. \\nThe National Highway Traffic Safety Administration,14 through its rigorous standards and independent \\nevaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to \\ninnovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate \\nrequirements on drivers, such as slowing down near schools or playgrounds.16\\nFrom large companies to start-ups, industry is providing innovative solutions that allow \\norganizations to mitigate risks to the safety and efficacy of AI systems, both before \\ndeployment and through monitoring over time.17 These innovative solutions include risk \\nassessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing \\nmonitoring, documentation procedures specific to model assessments, and many other strategies that aim to \\nmitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety \\nand effectiveness concerns. \\nThe Office of Management and Budget (OMB) has called for an expansion of opportunities \\nfor meaningful stakeholder engagement in the design of programs and services. OMB also \\npoints to numerous examples of effective and proactive stakeholder engagement, including the Community-\\nBased Participatory Research Program developed by the National Institutes of Health and the participatory \\ntechnology assessments developed by the National Oceanic and Atmospheric Administration.18\\nThe National Institute of Standards and Technology (NIST) is developing a risk \\nmanagement framework to better manage risks posed to individuals, organizations, and \\nsociety by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for \\nvoluntary use to help incorporate trustworthiness considerations into the design, development, use, and \\nevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \\ninput. The NIST framework aims to foster the development of innovative approaches to address \\ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy,'),\n",
      "                      Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 16, '_id': '779dcfed70684bd6a3f6b52203af87b4', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content='SAFE AND EFFECTIVE \\nSYSTEMS \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\n• AI-enabled “nudification” technology that creates images where people appear to be nude—including apps that\\nenable non-technical users to create or alter images of individuals without their consent—has proliferated at an\\nalarming rate. Such technology is becoming a common form of image-based abuse that disproportionately\\nimpacts women. As these tools become more sophisticated, they are producing altered images that are increasing -\\nly realistic and are difficult for both humans and AI to detect as inauthentic. Regardless of authenticit y, the expe -\\nrience of harm to victims of non-consensual intimate images can be devastatingly real—affecting their personal\\nand professional lives, and impacting their mental and physical health.10\\n• A company installed AI-powered cameras in its delivery vans in order to evaluate the road safety habits of its driv -\\ners, but the system incorrectly penalized drivers when other cars cut them off or when other events beyond\\ntheir control took place on the road. As a result, drivers were incorrectly ineligible to receive a bonus.11\\n17'),\n",
      "                      Document(metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 1, '_id': 'fb6e9883d9994c04acb499315b05bc51', '_collection_name': 'e44fae33793847efb8d68251c051177e'}, page_content='existing policies and safeguards that govern automated systems, including, for example, Executive Order 13960, \\nPromoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020).  \\nThis white paper recognizes that national security (which includes certain law enforcement and \\nhomeland security activities) and defense activities are of increased sensitivity and interest to our nation’s \\nadversaries and are often subject to special requirements, such as those governing classified information and \\nother protected data. Such activities require alternative, compatible safeguards through existing policies that \\ngovern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and \\nResponsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and \\nFramework. The implementation of these policies to national security and defense activities can be informed by \\nthe Blueprint for an AI Bill of Rights where feasible. \\nThe Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, or \\ndefense, substantive or procedural, enforceable at law or in equity by any party against the United States, its \\ndepartments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a \\nwaiver of sovereign immunity. \\nCopyright Information \\nThis document is a work of the United States Government and is in the public domain (see 17 U.S.C. §105). \\n2')]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "response2 = retrieval_augmented_qa_chain.invoke({\"query\": \"What are some of the principles for ensuring safety in relation to AI?\"})\n",
    "pprint.pp(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/nrm5rwy968d4jz2sck87745r0000gn/T/ipykernel_1349/3410574850.py:28: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  qdrant_vector_store = Qdrant(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['42967d1c5e8d436681fd7fe82ef8fe72',\n",
       " 'be6093b03ad44c949a5415380ef3e264',\n",
       " '033a2305072a48c59208700f6b06e7d9',\n",
       " '539b7e2a684b4742b5c02dd977bcc607',\n",
       " '914d39561c774dd580b1c2a52915ee74',\n",
       " '086b1048dde54307991cbf8c4e47dfef',\n",
       " 'b72adfafeb1149bab191188bf5f4bc86',\n",
       " '478d0e631c9448c7ac53e373426e365a',\n",
       " 'd3a1c6628c9b42c791a8ee10aeb396bc',\n",
       " 'd7d7790101d04ccdb50be77476caecbf',\n",
       " '3599414751554243b435bd66c5b0a1d3',\n",
       " '507e7f1f845b434f9432ec81c9b27020',\n",
       " '1e6484fdbf68426eb4b31bfb2cd607d8',\n",
       " 'be239b0154704f369299a5c40bb20ce8',\n",
       " 'b59125432a7949f5805c3402607e479e',\n",
       " 'ae5b095fecdf4d74bc0bb034ec4224c2',\n",
       " '3a457d4d52884167b01185a102877f8e',\n",
       " '16e30cd50d054a2ab10041b44e878a0b',\n",
       " 'bfd3a5e89e6d4b4b895f1a5b349a8f8d',\n",
       " '84e135efc69f405eb1f8f67b06f0e9c0',\n",
       " '06888bbde10f483581de72e3c7625e41',\n",
       " '07ebb8444d964e0bb0c91f1f2afbf1b9',\n",
       " 'c978848a6cd14f00bf5b00c605b1b225',\n",
       " '083f37b9339f4ad696397a4c8fccd9e1',\n",
       " 'e91094220dda493e8e30683c4e55b9d5',\n",
       " '4046c774b4884db0b966eb7ca515c479',\n",
       " '64aa8185444a4d2fafeb4faf5d9a3546',\n",
       " '4819cfcaff644a2ca5303fb89a5f4d5d',\n",
       " 'c80c4c9682b7450a810c0d61c9650746',\n",
       " '68a9e5f766844afd87765d251e1d4753',\n",
       " '7f3496c9363043c6b4d5c1f3b69326c6',\n",
       " '02b9b44e871b4b5d9926b5a0ed9e25e3',\n",
       " 'e3acf5fcff36479f982eabe5cdef3768',\n",
       " '53a1c42b1bc64a9ebfd57dd5700a076c',\n",
       " 'fa199bc142ab4cfb86ee3c70cd70aca3',\n",
       " 'f1afdd4f417f4618bc6a654f8ee3ef8d',\n",
       " 'c44d7a8b619547da8990edc60e1ca33a',\n",
       " 'c695193686f547f78444a05960792ca4',\n",
       " '115598dc99644aedb434eae2b56b0394',\n",
       " 'a3900e1640e14629b9db87e3e1e3e29f',\n",
       " 'a7458a9b229247d6ae4060ff1545c2a4',\n",
       " '21a53d5f1a9940fdad268084658b9791',\n",
       " 'bb3b1b51caa94b9bb7deec53c5dbb76b',\n",
       " '3145379f43984b84bc58cf7268e4953d',\n",
       " '9460f1e1ee69470aa2eeae9bfe623d5e',\n",
       " '723b155505784eaba5aa2cfb6cb42b40',\n",
       " '7b478e3f47de4042a3bf57ce1a44233f',\n",
       " 'b26dc4a4c58c4453a2667dfd348ad9ce',\n",
       " '41e0ad22e21140e1bd68e04d280dd749',\n",
       " '250a208f7ee8493b9dcb18d589435e49',\n",
       " '4d537077b97141cab47e6298def912d8',\n",
       " '84221133ffe34aeabc780dba2293d8ce',\n",
       " '9f69b2cee57d4b64b4641a1e94b0746f',\n",
       " '10d962f3a61d4a40b614612aaf72e96a',\n",
       " '87e585f02912402188db87e76c305d4a',\n",
       " 'e9d740df584d479caf104b5ab3479e02',\n",
       " '43dc8fbe754f4a50a3b68d80ee8fa270',\n",
       " '3c13467c0a464c4f971b4885311e1198',\n",
       " '79366da82f2f4381a77a1d65d58d3b06',\n",
       " '4c7507d785d1480da1f1733004676c2d',\n",
       " '561d8f37469e496c9b255b1d451489c8',\n",
       " '257b8cf8df344e1b808b5d72af0b73c6',\n",
       " '0ede1f6b9f3043da81bf1cb78fe0cd6c',\n",
       " '718cbbfed92f475595952e38ca71bfe3',\n",
       " '042967782fd34cd5a3dbc4aa29bba57d',\n",
       " 'c89093ce907d45d1a35909c9978d12d5',\n",
       " '59b7b35ee1ca4c18bd63953f49f50478',\n",
       " '1343673f070441d684ad18fe8c6d3ed8',\n",
       " '8a82cf0eea64422d887315224f430bc0',\n",
       " 'e1dd025a4c8a4334b45c26b237c69196',\n",
       " '6bc24ed73ee746d6b4cb24475846d2b0',\n",
       " 'cd8c47e88a7e4aa2a2a27a029d1c48af',\n",
       " '9596a4b11d054ca8820091bbe19120d9',\n",
       " '15838131e5154d618d6e46601d7286b0',\n",
       " '0290f718529e41218c24bb9beda14f68',\n",
       " '1624af8108694f7aa7af0df81853fa16',\n",
       " '4919d3ce6a204698a62f56cbae83b247',\n",
       " 'b3dbb7c2e18d46688e46fee840592722',\n",
       " '48d047196bb54a47acc2d06afe56ac35',\n",
       " 'e793424457cb40b78a35f84772fe0f7c',\n",
       " '240cf04269bf4481949d20c152594920',\n",
       " '6015e33d8569423c99b5000abfc93c47',\n",
       " '4519414c9e4b47b1901bf0e08e47feac',\n",
       " '77221e63d4924e3cadae78f8801d8e4e',\n",
       " '59db10097367475c9aac934116a194d2',\n",
       " '45901ca0e3624ba9a4ef93840099f29f',\n",
       " '6d2896d5a7a0467ca0a92ee04edab106',\n",
       " '73c4c321bd8a4d6f95c5d07f4f15f642',\n",
       " '2278461d99d54124879dd1f8f6ccf67f',\n",
       " '8f11939e72364b35b11a91da629eee9a',\n",
       " '9a1e5913c4a94fc39c412e989b2eb06f',\n",
       " '7fe3d9b317e34988b40bd2a540c9aa92',\n",
       " '30ac00f8690c43ea86acd57097a75691',\n",
       " 'fad06ec781664237a6186d75df28b850',\n",
       " 'af7980b7c8524076aac30249424399c3',\n",
       " '82be841b565343e496e05662d16e8aaf',\n",
       " 'abe057c3fcbe4f258c20ec2663bc4762',\n",
       " 'e35b9422b58741a2a9d42d8e67b5f57f',\n",
       " '96518ba1b35243eda0994dd02776e4fb',\n",
       " 'e0b59bcec49d4abe82c4a979b0542fb8',\n",
       " '351560bad2f648dba1083efd2599afd1',\n",
       " '11e8dad583854ad4b02cb87cb7640c08',\n",
       " '91b423c36241423bbdac5a0690113032',\n",
       " 'd3a7b7616f8e4acaba17783403b8fe50',\n",
       " 'eba87c7f6d334cdc95b9cc33b577d74f',\n",
       " 'b4b998324ea54cf08a58fb687adb2690',\n",
       " '3890cf4ef1b6401ea6bc0f60f43ef3ae',\n",
       " 'b3aa6a1748cd44da8539f2ab954a1002',\n",
       " '07874b9009a94d67ae8be395de598fab',\n",
       " '085a7dd6df734ff799c5bd307031e5f8',\n",
       " 'ed0490029a5f442495cae3ce8ffaebd2',\n",
       " 'a757077d0e2b4901b14fe65fa494029b',\n",
       " '75c4528b88e043d6bbb1daa5545a7d15',\n",
       " 'd1c42b8fdc104cc79d2a0106d174f310',\n",
       " 'df50747104e849b7879346e021cae4ed',\n",
       " 'a5270974d9e64b90b181644a076e4505',\n",
       " 'e4f2cabda5c34992bdb4b974fe743a71',\n",
       " 'df891abda9cd47018ecd119771ae6e7a',\n",
       " '54212426fe3d4b0aa7c27e5ce679e7f1',\n",
       " '1fa00baf46aa43b6bfb7c233080eadbb',\n",
       " '71a2ad00c3ae40d986a0e55fdfa13e33',\n",
       " 'b2d291640101469dbe208381aa8d6465',\n",
       " '93099b452b10453f9e07393988c8b8f2',\n",
       " '0eeb8af6d85e452cbf04fac2298393d3',\n",
       " '5ebd39a09a8144c8b7c1648c26d60216',\n",
       " '2a4e97db4318481298134e6a8c52407d',\n",
       " '081488adbea34ae48f18a0759c15df1c',\n",
       " 'b8f1d7d8391d49ec9d296b046b399879',\n",
       " 'd201a2d77b6f42cebe890d37a1473cf6',\n",
       " '7be31a073ede4695936dbcf8ed904eba',\n",
       " 'ce5b00295d634c3da6ba0bfa0d2ec1cb',\n",
       " 'd63b3fc7c30244d3a9d1fd32430eb96b',\n",
       " '88bd7d4f14f9410aa5768e779f51bf27',\n",
       " '9cca64139427439db116d202c306c60b',\n",
       " '27d8dbdc136e453e9badc09febf0e893',\n",
       " '22bba9a759164a24a3d07bd7487b4585',\n",
       " '8128f63def8e45bda18ae951a427b371',\n",
       " '706a5bfbc8874d91bd18a36a6a3b5280',\n",
       " '286c84cb1f4d47c89cb25dbe24fb0703',\n",
       " 'c59aaecba8cf4105af88ff07b358ce68',\n",
       " 'fb46d7bcc1d44db295d0bfd15406db0a',\n",
       " 'd188afa45c224590bf894fbf32f7dca3',\n",
       " '605900516aac4d779fd6d2a8019f837d',\n",
       " '79483ea5c0874f36bce0510abb099033',\n",
       " '8b1ede4d7f76400a85102b7a5896efd4',\n",
       " '10f7c67c8ff6487d8a19a013704bd06c',\n",
       " '8a6b83d5f09c4cfc8b85a9138aab85c2',\n",
       " '94fbfd141a1b4d4387b409d9e29cf6a0',\n",
       " '22633ce5c4074c2a8117c5332bc8dd86',\n",
       " '683180f126c548a695c64dfe11eac32c',\n",
       " '09be5c683e574fe49102fb88a08cc54f',\n",
       " 'a6877159d61b4fec9c93e5f5e7079f31',\n",
       " 'fe3810fd2b2b4d01aa13e6f6b7ea89c8',\n",
       " '214b6ffea8344a2f81c432697529f463',\n",
       " 'adb9ca0af0f94bd1968c82dc670b517f']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Load the PDFs\n",
    "loader1 = PyPDFLoader(\"/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf\")\n",
    "loader2 = PyPDFLoader(\"/Users/joenoss/Downloads/NIST.AI.600-1.pdf\")\n",
    "pages1 = loader1.load_and_split()\n",
    "pages2 = loader2.load_and_split()\n",
    "\n",
    "# Combine pages from both PDFs\n",
    "all_pages = pages1 + pages2\n",
    "\n",
    "# Create the Qdrant client with in-memory storage\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create the collection\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=\"ai_documents\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# Create the OpenAI embeddings instance\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "qdrant_vector_store = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"ai_documents\",\n",
    "    embeddings=embedding_model,  # Use 'embeddings' instead of 'embedding_function'\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "qdrant_vector_store.add_documents(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant_vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2},\n",
    "    score_threshold=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='SAFE AND EFFECTIVE \n",
      "SYSTEMS \n",
      "HOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\n",
      "Real-life examples of how these principles can become reality, through laws, policies, and practical \n",
      "technical and sociotechnical approaches to protecting rights, opportunities, and access. \n",
      "Some U.S government agencies have developed specific frameworks for ethical use of AI \n",
      "systems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina -\n",
      "tion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the \n",
      "ethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence \n",
      "Ethical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \n",
      "security and defense activities.21 Similarl y, the U.S. Intelligence Community (IC) has developed the Principles \n",
      "of Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to \n",
      "develop and use AI in furtherance of the IC's mission, as well as an AI Ethics Framework to help implement \n",
      "these principles.22\n",
      "The National Science Foundation (NSF) funds extensive research to help foster the \n",
      "development of automated systems that adhere to and advance their safety, security and \n",
      "effectiveness. Multiple NSF programs support research that directly addresses many of these principles: \n",
      "the National AI Research Institutes23 support research on all aspects of safe, trustworth y, fai r, and explainable \n",
      "AI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe \n",
      "autonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25 \n",
      "program supports research on cybersecurity and privacy enhancing technologies in automated systems; the \n",
      "Formal Methods in the Field26 program supports research on rigorous formal verification and analysis of \n",
      "automated systems and machine learning, and the Designing Accountable Software Systems27 program supports \n",
      "research on rigorous and reproducible methodologies for developing software systems with legal and regulatory \n",
      "compliance in mind. \n",
      "Some state legislatures have placed strong transparency and validity requirements on \n",
      "the use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a \n",
      "cause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any \n",
      "pretrial risk assessment, before use in the state, first be \"shown to be free of bias against any class of \n",
      "individuals protected from discrimination by state or federal law\", that any locality using a pretrial risk \n",
      "assessment must first formally validate the claim of its being free of bias, that \"all documents, records, and \n",
      "information used to build or validate the risk assessment shall be open to public inspection,\" and that assertions \n",
      "of trade secrets cannot be used \"to quash discovery in a criminal matter by a party to a criminal case.\" \n",
      "22' metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 21, '_id': 'c80c4c9682b7450a810c0d61c9650746', '_collection_name': 'ai_documents'}\n",
      "page_content='SAFE AND EFFECTIVE \n",
      "SYSTEMS \n",
      "HOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\n",
      "Real-life examples of how these principles can become reality, through laws, policies, and practical \n",
      "technical and sociotechnical approaches to protecting rights, opportunities, and access. \n",
      "Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the \n",
      "Federal Government requires that certain federal agencies adhere to nine principles when \n",
      "designing, developing, acquiring, or using AI for purposes other than national security or \n",
      "defense. These principles—while taking into account the sensitive law enforcement and other contexts in which \n",
      "the federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and \n",
      "respectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) \n",
      "safe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar -\n",
      "ent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. \n",
      "Affected agencies across the federal government have released AI use case inventories13 and are implementing \n",
      "plans to bring those AI systems into compliance with the Executive Order or retire them. \n",
      "The law and policy landscape for motor vehicles shows that strong safety regulations—and \n",
      "measures to address harms when they occur—can enhance innovation in the context of com-\n",
      "plex technologies. Cars, like automated digital systems, comprise a complex collection of components. \n",
      "The National Highway Traffic Safety Administration,14 through its rigorous standards and independent \n",
      "evaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to \n",
      "innovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate \n",
      "requirements on drivers, such as slowing down near schools or playgrounds.16\n",
      "From large companies to start-ups, industry is providing innovative solutions that allow \n",
      "organizations to mitigate risks to the safety and efficacy of AI systems, both before \n",
      "deployment and through monitoring over time.17 These innovative solutions include risk \n",
      "assessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing \n",
      "monitoring, documentation procedures specific to model assessments, and many other strategies that aim to \n",
      "mitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety \n",
      "and effectiveness concerns. \n",
      "The Office of Management and Budget (OMB) has called for an expansion of opportunities \n",
      "for meaningful stakeholder engagement in the design of programs and services. OMB also \n",
      "points to numerous examples of effective and proactive stakeholder engagement, including the Community-\n",
      "Based Participatory Research Program developed by the National Institutes of Health and the participatory \n",
      "technology assessments developed by the National Oceanic and Atmospheric Administration.18\n",
      "The National Institute of Standards and Technology (NIST) is developing a risk \n",
      "management framework to better manage risks posed to individuals, organizations, and \n",
      "society by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for \n",
      "voluntary use to help incorporate trustworthiness considerations into the design, development, use, and \n",
      "evaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\n",
      "driven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \n",
      "input. The NIST framework aims to foster the development of innovative approaches to address \n",
      "characteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy,' metadata={'source': '/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 20, '_id': '64aa8185444a4d2fafeb4faf5d9a3546', '_collection_name': 'ai_documents'}\n"
     ]
    }
   ],
   "source": [
    "retrieved_documents = retriever.invoke(\"What are some of the principles for ensuring safety in relation to AI?\")\n",
    "for doc in retrieved_documents:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Use the ChatOpenAI model we set up earlier\n",
    "primary_qa_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Use the chat prompt we defined earlier\n",
    "system_template = \"\"\"You are an AI assistant specialized in answering questions about the AI Regulations, Policies, and Frameworks. \n",
    "Your responses should be based on the provided context. If the information isn't in the context, say you don't know.\n",
    "Always strive for accuracy and clarity in your answers.\"\"\"\n",
    "\n",
    "human_template = \"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a concise and accurate answer based on the given context.\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "# Set up the retrieval augmented QA chain\n",
    "retrieval_augmented_qa_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, \n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=lambda x: \"\\n\\n\".join([doc.page_content for doc in x[\"context\"]]))\n",
    "    | {\n",
    "        \"response\": chat_prompt | primary_qa_llm | StrOutputParser(), \n",
    "        \"context\": itemgetter(\"context\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'response': 'The riskiness of AI is defined by the unique and exacerbated '\n",
      "              'risks associated with the development and use of Generative AI '\n",
      "              '(GAI). These risks can vary based on characteristics such as '\n",
      "              'the GAI model or system architecture, training mechanisms, data '\n",
      "              'types used, and the context of application. Organizations may '\n",
      "              'tailor their measurement of GAI risks according to these '\n",
      "              'characteristics and allocate risk management resources based on '\n",
      "              'the severity and likelihood of negative impacts. Risks can be '\n",
      "              'categorized into technical/model risks, misuse by humans, and '\n",
      "              'ecosystem/societal risks, with some risks being cross-cutting '\n",
      "              'between these categories. Additionally, some GAI risks are '\n",
      "              'unknown or difficult to estimate due to the complexity and '\n",
      "              'scale of GAI systems, as well as challenges in visibility into '\n",
      "              'training data and the current state of AI measurement and '\n",
      "              'safety.',\n",
      "  'context': '57 National Institute of Standards and Technology (2023) AI Risk '\n",
      "             'Management Framework, Appendix B: \\n'\n",
      "             'How AI Risks Diﬀer from Traditional Software Risks . \\n'\n",
      "             'https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B  \\n'\n",
      "             'National Institute of Standards and Technology  (2023) AI RMF '\n",
      "             'Playbook . \\n'\n",
      "             'https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook  \\n'\n",
      "             'National Institue of Standards and Technology (2023) Framing '\n",
      "             'Risk \\n'\n",
      "             'https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1- '\n",
      "             'sec-risk \\n'\n",
      "             'National Institu te of Standards and Technology (2023) The '\n",
      "             'Language of Trustworthy AI: An In- Depth \\n'\n",
      "             'Glossary of Terms '\n",
      "             'https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary  \\n'\n",
      "             'National Institue of Standards and Technology (2022) Towards a '\n",
      "             'Standard for Identifying and Managing \\n'\n",
      "             'Bias in Artiﬁcial Intelligence '\n",
      "             'https://www.nist.gov/publications/towards -standard -identifying '\n",
      "             '-and-\\n'\n",
      "             'managing- bias-artiﬁcial -intelligence  \\n'\n",
      "             'Northcutt, C. et al. (2021) Pervasive Label Errors in Test Sets '\n",
      "             'Destabilize Machine Learning Benchmarks.  \\n'\n",
      "             'arXiv . https://arxiv.org/pdf/2103.14749  \\n'\n",
      "             'OECD (2023) \"Advancing accountability in AI: Governing and '\n",
      "             'managing risks throughout the lifecycle for \\n'\n",
      "             'trustworthy AI\", OECD Digital Economy Papers , No. 349, OECD '\n",
      "             'Publishing,  Paris . \\n'\n",
      "             'https://doi.org/10.1787/2448f04b- en \\n'\n",
      "             'OECD (2024) \"Deﬁning AI incidents and related terms\" OECD '\n",
      "             'Artiﬁcial Intelligence Papers , No. 16, OECD \\n'\n",
      "             'Publishing, Paris . https://doi.org/10.1787/d1a8d965- en \\n'\n",
      "             'OpenAI  (2023) GPT-4 System Card . '\n",
      "             'https://cdn.openai.com/papers/gpt -4-system -card.pdf  \\n'\n",
      "             'OpenAI  (2024) GPT-4 Technical Report. '\n",
      "             'https://arxiv.org/pdf/2303.08774  \\n'\n",
      "             'Padmakumar, V. et al. (2024) Does writing with language models '\n",
      "             'reduce content diversity?  ICLR . \\n'\n",
      "             'https://arxiv.org/pdf/2309.05196  \\n'\n",
      "             'Park,  P.  et. al. (2024)  AI deception: A survey of  examples, '\n",
      "             'risks, and potential solutions. Patterns, 5(5).  \\n'\n",
      "             'arXiv . https://arxiv.org/pdf/2308.14752  \\n'\n",
      "             'Partnership on AI  (2023) Building a Glossary for Synthetic '\n",
      "             'Media Transparency Methods, Part 1: Indirect \\n'\n",
      "             'Disclosure . https://partnershiponai.org/glossary -for-synthetic '\n",
      "             '-media- transparency -methods -part-1-\\n'\n",
      "             'indirect -disclosure/  \\n'\n",
      "             'Qu, Y . et al. (2023) Unsafe Diﬀusion: On the Generation of '\n",
      "             'Unsafe Images and Hateful Memes From Text -\\n'\n",
      "             'To-Image Models.  arXiv . https://arxiv.org/pdf/2305.13873  \\n'\n",
      "             'Rafat, K. et al. (2023) Mitigating carbon footprint for '\n",
      "             'knowledge distillation based deep learning model \\n'\n",
      "             'compression. PLOS One . '\n",
      "             'https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668  \\n'\n",
      "             'Said, I. et al. (2022) Nonconsensual Distribution of Intimate '\n",
      "             'Images: Exploring the Role of Legal Attitudes in Victimization '\n",
      "             'and Perpetration. Sage. \\n'\n",
      "             'https://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47- '\n",
      "             '08862605221122834\\n'\n",
      "             ' \\n'\n",
      "             'Sandbrink, J. (2023) Artiﬁcial intelligence and biological '\n",
      "             'misuse: Diﬀerentiating risks of language models and biological '\n",
      "             'design tools. arXiv . https://arxiv.org/pdf/2306.13952\\n'\n",
      "             '\\n'\n",
      "             '3 the abuse, misuse, and unsafe repurposing by humans '\n",
      "             '(adversarial or not ), and  others result \\n'\n",
      "             'from interactions between a human and an AI system.   \\n'\n",
      "             '• Time  scale: GAI risks  may materialize  abruptly or  across '\n",
      "             'extended periods . Example s include \\n'\n",
      "             'immediate  (and/or prolonged) emotional harm  and potential '\n",
      "             'risks to physical safety  due to the \\n'\n",
      "             'distribution of harmful deepfake images , or the lo ng-term '\n",
      "             'eﬀect of disinformation on soci etal \\n'\n",
      "             'trust in public  institutions . \\n'\n",
      "             'The presence of risks  and where they fall along the dimensions '\n",
      "             'above will vary depending on the \\n'\n",
      "             'characteristics of the GAI model , system, or use case at hand. '\n",
      "             'These characteristics include but are not \\n'\n",
      "             'limited to GAI model or system architecture, training '\n",
      "             'mechanisms  and libraries , data types  used for \\n'\n",
      "             'training or ﬁne -tuning , levels of model access or availability '\n",
      "             'of model weights,  and application or use \\n'\n",
      "             'case context. \\n'\n",
      "             'Organizations may choose to tailor how they measure  GAI risks  '\n",
      "             'based on  these characteristics . They may \\n'\n",
      "             'additionally wish to  allocate risk management resources '\n",
      "             'relative to the severity and likelihood of \\n'\n",
      "             'negative impact s, including where and how these risks manifest '\n",
      "             ', and their direct and material impacts  \\n'\n",
      "             'harms in the context of GAI use. Mitigations for model or '\n",
      "             'system  level risks may diﬀer from mitigations  \\n'\n",
      "             'for use-case or ecosystem level risks.  \\n'\n",
      "             'Importantly, some GAI risks are un known , and are therefore '\n",
      "             'diﬃcult to properly scope or evaluate given  \\n'\n",
      "             'the uncertaint y about potential GAI scale, complexity, and '\n",
      "             'capabilities. Other risks may be known but  \\n'\n",
      "             'diﬃcult to estimate  given the wide range of GAI stakeholders, '\n",
      "             'uses, inputs, and outputs . Challenges  with \\n'\n",
      "             'risk estimation are aggravated by a lack of visibility into GAI '\n",
      "             'training data, and the generally immature \\n'\n",
      "             'state of the science of AI measurement and safety  today . This '\n",
      "             'document focuses on risks for which there \\n'\n",
      "             'is an existing empirical evidence base at the time this proﬁle '\n",
      "             'was written ; for example,  speculative risks \\n'\n",
      "             'that may potentially arise in more advanced, future GAI systems '\n",
      "             'are not considered . Future updates may \\n'\n",
      "             'incorporate additional risks or provide further details on the '\n",
      "             'risks identiﬁed below.  \\n'\n",
      "             'To guide organizations in identifying and managing GAI risks, a '\n",
      "             'set of risks unique to or exacerbated by \\n'\n",
      "             'the development and use of GAI are deﬁned below.5 Each risk is  '\n",
      "             'labeled according to  the outcome , \\n'\n",
      "             'object,  or source of the risk  (i.e., some are risks “to ” a '\n",
      "             'subject  or domain  and others are  risks  “of” or \\n'\n",
      "             '“from” an issue or  theme ). These risks provide a lens  through '\n",
      "             'which organizations can frame and execute \\n'\n",
      "             'risk management eﬀorts.  To help streamline risk management '\n",
      "             'eﬀorts, each risk is mapped in Section 3  \\n'\n",
      "             '(as well as in tables in Appendix B)  to relevant Trustworthy AI '\n",
      "             'Characteristics  identiﬁed in the AI RMF .  \\n'\n",
      "             ' \\n'\n",
      "             ' \\n'\n",
      "             '5 These risks can be further categorized by organizations '\n",
      "             'depending on their unique approaches to risk deﬁnition \\n'\n",
      "             'and management. One possible way to further categorize these '\n",
      "             'risks, derived in part from the UK’s International \\n'\n",
      "             'Scientiﬁc Report on the Safety of Advanced AI , could be: 1 ) '\n",
      "             'Technical / Model risks (or risk from malfunction): \\n'\n",
      "             'Confabulation; Dangerous or Violent Recommendations; Data '\n",
      "             'Privacy; Value Chain and Component Integration; \\n'\n",
      "             'Harmful Bias, and Homogenization ; 2) Misuse by humans (or '\n",
      "             'malicious use):  CBRN Information  or Capabilities ; \\n'\n",
      "             'Data Privacy; Human -AI Conﬁguration; Obscene, Degrading, and/or '\n",
      "             'Abusive Content; Information Integrity;  \\n'\n",
      "             'Information Security; 3) Ecosystem / societal risks (or systemic '\n",
      "             'risks) : Data Privacy; Environmental; Intellectual \\n'\n",
      "             'Property . We also note that some risks are cross -cutting '\n",
      "             'between these categories.'}]\n"
     ]
    }
   ],
   "source": [
    "question = \"How do we define the riskiness of AI?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "pprint.pp([result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 155 test document pages\n"
     ]
    }
   ],
   "source": [
    "# Reload the documents for testing purposes\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the PDFs\n",
    "test_loader1 = PyPDFLoader(\"/Users/joenoss/Downloads/Blueprint-for-an-AI-Bill-of-Rights.pdf\")\n",
    "test_loader2 = PyPDFLoader(\"/Users/joenoss/Downloads/NIST.AI.600-1.pdf\")\n",
    "test_pages1 = test_loader1.load_and_split()\n",
    "test_pages2 = test_loader2.load_and_split()\n",
    "\n",
    "# Combine pages from both PDFs\n",
    "test_all_pages = test_pages1 + test_pages2\n",
    "\n",
    "# Create the Qdrant client with in-memory storage for testing\n",
    "test_qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create the collection for testing\n",
    "test_qdrant_client.create_collection(\n",
    "    collection_name=\"test_ai_documents\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# Create the OpenAI embeddings instance\n",
    "test_embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create the vector store for testing\n",
    "test_qdrant_vector_store = Qdrant(\n",
    "    client=test_qdrant_client,\n",
    "    collection_name=\"test_ai_documents\",\n",
    "    embeddings=test_embedding_model,\n",
    ")\n",
    "\n",
    "# Add documents to the test vector store\n",
    "test_qdrant_vector_store.add_documents(test_all_pages)\n",
    "\n",
    "# Create a new retriever for testing\n",
    "test_retriever = test_qdrant_vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2},\n",
    "    score_threshold=0.7\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(test_all_pages)} test document pages\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script langchain-server is installed in '/Users/joenoss/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragas 0.1.20 requires langchain-core<0.3, but you have langchain-core 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -q --user langchain langchain-openai langchain_core langchain-community langchainhub openai langchain-qdrant qdrant-client pymupdf pandas ragas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
